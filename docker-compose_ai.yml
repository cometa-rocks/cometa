services:
  redis:
    image: redis:7.2.0-alpine
    container_name: cometa_redis
    volumes:
      - redis_data:/data
    networks:
      - testing
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
    restart: always

  ollama:
    build: ./cometa_ai
    container_name: cometa_ollama
    ports: 
      - "8083:8083"
    volumes:
      - ./cometa_ai:/app
      - ./data/ollama:/root/.ollama/models
    working_dir: /app 
    command: bash /app/start.sh 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Or specify a number like 1 if you want to limit GPU usage to a single GPU
              capabilities: [gpu]
    
    runtime: nvidia  # Use NVIDIA's runtime
    environment:
      OLLAMA_HOST: "0.0.0.0:8083"
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      NAME: ollma
      REDIS_HOST: "redis"
      REDIS_PORT: 6379
      REDIS_DB: 0
      REDIS_DEFAULT_TIMEOUT: 7500
    networks:
      - testing
    restart: always

  # producer:
  #   build: ./producer
  #   volumes:
  #     - ./producer:/app
  #   depends_on:
  #     - redis
  #   environment:
  #     # REDIS_HOST=redis
  #     REDIS_HOST: "redis"
  #     REDIS_PORT: 6379
  #     REDIS_DB: 0
  #     REDIS_DEFAULT_TIMEOUT: 7500
  #   networks:
  #     - testing
  #   restart: always
    


networks:
  testing:
    driver: "bridge"

volumes:
  redis_data:
